{
  "num_epochs": 2,
  "batch_size": 32,
  "learning_rate": 0.001,
  "dropout_ratio": 0.1,
  "warmup_steps": 0,
  "max_sequence_length": 128,
  "num_heads": 4,
  "num_encoder_layers": 2,
  "num_decoder_layers": 2,
  "model_dim": 128,
  "ff_dim": 512,
  "top_k": 10,
  "temperature": 0.7,
  "early_stopping_patience": 5
}
